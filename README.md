# Meetup ETL Pipeline
---
Este proyecto implementa un pipeline de procesamiento de datos utilizando `Apache Airflow`, `Snowflake`, y `AWS S3`. Automatiza la transformaciÃ³n y exportaciÃ³n de datos desde archivos CSV hacia una arquitectura moderna de datos en la nube.

## ğŸ“‚ Estructura del Proyecto

PROJECT_5/
â”œâ”€â”€ Data/ # Archivos CSV originales del dataset de Meetup
â”œâ”€â”€ Pipeline/ # ConfiguraciÃ³n y orquestaciÃ³n de Airflow
â”‚ â”œâ”€â”€ dags/ # DAGs de Airflow
â”‚ â”œâ”€â”€ config/ # ConfiguraciÃ³n de Airflow
â”‚ â”œâ”€â”€ plugins/ # Funciones externas y configuraciones
â”‚ â”œâ”€â”€ logs/ # Logs generados por Airflow
â”‚ â”œâ”€â”€ .env.example # Variables de entorno de ejemplo
â”‚ â”œâ”€â”€ docker-compose.yaml # Entorno Docker para Airflow
â”‚ â””â”€â”€ *.json, *.sql # Scripts SQL y polÃ­ticas AWS
â”œâ”€â”€ Querys/ # Scripts SQL por tabla (para Snowflake)
â””â”€â”€ README.md

## âš™ï¸ TecnologÃ­as utilizadas

- Apache Airflow (en Docker)
- Snowflake
- Amazon S3
- Python 3.12
- Slack (para alertas)

## ğŸ” DAG de Airflow

El DAG ejecuta las siguientes tareas cada 15 minutos:

1. **Crear tablas** en Snowflake (si no existen)
2. **Transformar datos** desde tablas RAW a STG
3. **Cargar datos nuevos** usando `MERGE`
4. **Notificar en Slack** al finalizar (Ã©xito o error)
5. **Exportar tabla final a S3** en formato CSV

## ğŸ“¤ ExportaciÃ³n a S3

Snowflake exporta la tabla `MEETUP_FINAL` al bucket `s3://meetup-pipeline-output/` utilizando una `STORAGE INTEGRATION`.

## ğŸ” Seguridad

Los siguientes archivos estÃ¡n ignorados por `.gitignore` para proteger informaciÃ³n sensible:

- `.env`
- Archivos `.json` con polÃ­ticas AWS
- Claves y scripts privados

## ğŸš€ CÃ³mo iniciar

1. Clona el repositorio
2. Copia el archivo `.env.example` a `.env` y completa tus credenciales
3. Ejecuta el entorno Airflow:

```bash
cd Pipeline
docker compose up --build